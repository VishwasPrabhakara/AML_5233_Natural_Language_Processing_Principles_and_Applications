{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " Write a program that analyzes the sentiment of a piece of text by takeing a piece of text as input from the user, analyzes the sentiment of the text, and then displays whether the overall sentiment is positive, negative, or neutral"
      ],
      "metadata": {
        "id": "RYus03PnMxbC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "def analyze_sentiment(text):\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    if polarity > 0:\n",
        "        return \"Positive\"\n",
        "    elif polarity < 0:\n",
        "        return \"Negative\"\n",
        "    else:\n",
        "        return \"Neutral\"\n",
        "\n",
        "\n",
        "text = input(\"Enter a piece of text: \")\n",
        "sentiment = analyze_sentiment(text)\n",
        "print(f\"\\nSentiment: {sentiment}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpqxavMIMxAE",
        "outputId": "b999ee61-6440-4d3a-b97f-d777e3d8ca5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a piece of text: The movie was terrible, I hated it.\n",
            "\n",
            "Sentiment: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-of-Speech Tagging: Use a library like NLTK or spaCy to perform part-of-speech tagging on\n",
        "a sentence. Print out the words along with their corresponding parts of speech."
      ],
      "metadata": {
        "id": "dDVypSxeOil2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RP-tV3BgMjS1",
        "outputId": "3c724f35-5ea9-4d83-f4f1-8070ebadcaf1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter a sentence: The quick brown fox jumps over the lazy dog, demonstrating agility and speed as it traverses the terrain with ease.\n",
            "\n",
            "Part-of-Speech Tagging:\n",
            "The: DT\n",
            "quick: JJ\n",
            "brown: NN\n",
            "fox: NN\n",
            "jumps: VBZ\n",
            "over: IN\n",
            "the: DT\n",
            "lazy: JJ\n",
            "dog: NN\n",
            ",: ,\n",
            "demonstrating: VBG\n",
            "agility: NN\n",
            "and: CC\n",
            "speed: NN\n",
            "as: IN\n",
            "it: PRP\n",
            "traverses: VBZ\n",
            "the: DT\n",
            "terrain: NN\n",
            "with: IN\n",
            "ease: NN\n",
            ".: .\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "\n",
        "def pos_tagging(sentence):\n",
        "    words = word_tokenize(sentence)\n",
        "    tagged_words = pos_tag(words)\n",
        "    return tagged_words\n",
        "\n",
        "\n",
        "sentence = input(\"Enter a sentence: \")\n",
        "tagged_words = pos_tagging(sentence)\n",
        "print(\"\\nPart-of-Speech Tagging:\")\n",
        "for word, pos in tagged_words:\n",
        "  print(f\"{word}: {pos}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF IDF EXAMPLE"
      ],
      "metadata": {
        "id": "LC4W4kFfPtZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def calculate_tfidf(documents):\n",
        "    tfidf_vectorizer = TfidfVectorizer()\n",
        "    tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "    return tfidf_matrix, tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "documents = [\n",
        "        \"The cat sits on the mat\",\n",
        "        \"The dog plays in the garden\",\n",
        "        \"Birds chirp in the morning\"\n",
        "    ]\n",
        "tfidf_matrix, feature_names = calculate_tfidf(documents)\n",
        "print(\"\\nTF-IDF Values:\")\n",
        "for i, doc in enumerate(documents):\n",
        "  print(f\"Document {i + 1}:\")\n",
        "  for j, word in enumerate(feature_names):\n",
        "    print(f\"{word}: {tfidf_matrix[i, j]}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCGsY2MhRG7L",
        "outputId": "ce6750a4-2365-4cae-f7ab-206d82ae919f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TF-IDF Values:\n",
            "Document 1:\n",
            "birds: 0.0\n",
            "cat: 0.4305184979719882\n",
            "chirp: 0.0\n",
            "dog: 0.0\n",
            "garden: 0.0\n",
            "in: 0.0\n",
            "mat: 0.4305184979719882\n",
            "morning: 0.0\n",
            "on: 0.4305184979719882\n",
            "plays: 0.0\n",
            "sits: 0.4305184979719882\n",
            "the: 0.5085423203783267\n",
            "Document 2:\n",
            "birds: 0.0\n",
            "cat: 0.0\n",
            "chirp: 0.0\n",
            "dog: 0.44839402160692654\n",
            "garden: 0.44839402160692654\n",
            "in: 0.3410152109911944\n",
            "mat: 0.0\n",
            "morning: 0.0\n",
            "on: 0.0\n",
            "plays: 0.44839402160692654\n",
            "sits: 0.0\n",
            "the: 0.5296574648148862\n",
            "Document 3:\n",
            "birds: 0.5046113401371842\n",
            "cat: 0.0\n",
            "chirp: 0.5046113401371842\n",
            "dog: 0.0\n",
            "garden: 0.0\n",
            "in: 0.3837699307603192\n",
            "mat: 0.0\n",
            "morning: 0.5046113401371842\n",
            "on: 0.0\n",
            "plays: 0.0\n",
            "sits: 0.0\n",
            "the: 0.2980315863446099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement a text classification system using Support Vector Machines (SVM) and the 20 Newsgroups dataset. Preprocess the text data by tokenizing, removing stopwords, and\n",
        "converting words to their base forms using lemmatization. Use TF-IDF vectorization to convert\n",
        "text into numerical features. Train an SVM classifier with a linear kernel and evaluate its\n",
        "performance using accuracy and a classification report."
      ],
      "metadata": {
        "id": "ZilcVw90Rl0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups_data = fetch_20newsgroups(subset='all', shuffle=True, remove=('headers', 'footers', 'quotes'))\n",
        "X, y = newsgroups_data.data, newsgroups_data.target\n"
      ],
      "metadata": {
        "id": "I3RKlG3iSJc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download NLTK resources if not already downloaded\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "    # Lemmatize words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "    # Join the tokens back into a single string\n",
        "    preprocessed_text = ' '.join(lemmatized_tokens)\n",
        "    return preprocessed_text\n",
        "\n",
        "# Preprocess the text data\n",
        "X_preprocessed = [preprocess_text(text) for text in X]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Convert text data into numerical features using TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=5000)\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "# Train an SVM classifier with a linear kernel\n",
        "svm_classifier = SVC(kernel='linear')\n",
        "svm_classifier.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svm_classifier.predict(X_test_tfidf)\n",
        "\n",
        "# Evaluate the performance\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=newsgroups_data.target_names))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZyYzhlYSWJV",
        "outputId": "936bcf37-ab4c-4b15-f984-8d2e5d964ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.6758620689655173\n",
            "Classification Report:\n",
            "                          precision    recall  f1-score   support\n",
            "\n",
            "             alt.atheism       0.48      0.57      0.52       151\n",
            "           comp.graphics       0.62      0.68      0.65       202\n",
            " comp.os.ms-windows.misc       0.65      0.63      0.64       195\n",
            "comp.sys.ibm.pc.hardware       0.56      0.63      0.59       183\n",
            "   comp.sys.mac.hardware       0.76      0.64      0.70       205\n",
            "          comp.windows.x       0.80      0.71      0.75       215\n",
            "            misc.forsale       0.74      0.70      0.72       193\n",
            "               rec.autos       0.43      0.71      0.54       196\n",
            "         rec.motorcycles       0.61      0.70      0.65       168\n",
            "      rec.sport.baseball       0.82      0.77      0.79       211\n",
            "        rec.sport.hockey       0.94      0.82      0.87       198\n",
            "               sci.crypt       0.87      0.68      0.76       201\n",
            "         sci.electronics       0.60      0.62      0.61       202\n",
            "                 sci.med       0.80      0.79      0.80       194\n",
            "               sci.space       0.69      0.74      0.71       189\n",
            "  soc.religion.christian       0.73      0.74      0.74       202\n",
            "      talk.politics.guns       0.72      0.71      0.71       188\n",
            "   talk.politics.mideast       0.81      0.68      0.74       182\n",
            "      talk.politics.misc       0.57      0.55      0.56       159\n",
            "      talk.religion.misc       0.43      0.26      0.33       136\n",
            "\n",
            "                accuracy                           0.68      3770\n",
            "               macro avg       0.68      0.67      0.67      3770\n",
            "            weighted avg       0.69      0.68      0.68      3770\n",
            "\n"
          ]
        }
      ]
    }
  ]
}